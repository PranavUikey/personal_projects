{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char - rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankur-singh/personal_projects/blob/master/Notebooks/char_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11vw_Roevl3s",
        "colab_type": "text"
      },
      "source": [
        "## Char-level RNN\n",
        "\n",
        "In this notebook, I'll construct a character-level RNN with PyTorch. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina. This model will be able to generate new text based on the text from the book!\n",
        "\n",
        "Below is the general architecture of the character-wise RNN.\n",
        "\n",
        "![](http://karpathy.github.io/assets/rnn/charseq.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkeIP-e10O2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcpE0W6k-yR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'anna.txt' # this is expected to give you an Error. You are suppose to specify your own text file's path."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xBhezPMrXj_",
        "colab_type": "text"
      },
      "source": [
        "### Defining Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_yR2LJKC2Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path):\n",
        "  with open(path) as f:\n",
        "    text = f.read()\n",
        "  return ''.join([i if ord(i) < 128 else ' ' for i in text]) # the preprocessing will depend on the use case \n",
        "\n",
        "\n",
        "def get_vocab(text):\n",
        "  chars = list(set(text)) \n",
        "  int2char = {i:c for i, c in enumerate(chars)}\n",
        "  char2int = {c:i for i, c in enumerate(chars)}\n",
        "  return chars, int2char, char2int\n",
        "\n",
        "\n",
        "def get_batches(enc_text, batch_size, seq_len):\n",
        "  n_batches = enc_text.shape[0] // (batch_size * seq_len)\n",
        "  enc_text = enc_text[:n_batches * batch_size * seq_len]\n",
        "  enc_text = enc_text.reshape(batch_size, -1)\n",
        "  \n",
        "  for i in range(0, enc_text.shape[1] - seq_len + 1, seq_len):\n",
        "    x = enc_text[:, i : i + seq_len]\n",
        "    y = np.zeros(x.shape)\n",
        "    \n",
        "    try:\n",
        "      y[:, : -1], y[:, -1] = x[:, 1:], enc_text[:, i + seq_len]  \n",
        "    except IndexError:\n",
        "      y[:, : -1], y[:, -1] = x[:, 1:], enc_text[:, 0]\n",
        "    \n",
        "    yield x,y\n",
        "    \n",
        "\n",
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCTFO5oICl2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = load_data(path)\n",
        "chars, int2char, char2int = get_vocab(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY0zWLKLFPWA",
        "colab_type": "code",
        "outputId": "205a99e9-4a81-42f8-8a8e-d75aaf3a4cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoded_text = np.array([char2int[c] for c in text])\n",
        "encoded_text.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1985223,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UD11ri3Dwdm",
        "colab_type": "code",
        "outputId": "5201f114-b13c-4944-d282-cf142fe0ed3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "chars[:10], list(int2char.items())[:10], list(char2int.items())[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['H', 'o', '4', 'g', 'y', '_', 'B', '5', 'l', 'k'],\n",
              " [(0, 'H'),\n",
              "  (1, 'o'),\n",
              "  (2, '4'),\n",
              "  (3, 'g'),\n",
              "  (4, 'y'),\n",
              "  (5, '_'),\n",
              "  (6, 'B'),\n",
              "  (7, '5'),\n",
              "  (8, 'l'),\n",
              "  (9, 'k')],\n",
              " [('H', 0),\n",
              "  ('o', 1),\n",
              "  ('4', 2),\n",
              "  ('g', 3),\n",
              "  ('y', 4),\n",
              "  ('_', 5),\n",
              "  ('B', 6),\n",
              "  ('5', 7),\n",
              "  ('l', 8),\n",
              "  ('k', 9)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OXYJEchD6OJ",
        "colab_type": "code",
        "outputId": "8e184dac-b30a-4b9a-9b7c-15d8d4ad6c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(text[:10])\n",
        "[char2int[c] for c in text[:10]]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25, 59, 80, 70, 69, 31, 68, 71, 34, 54]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxH0qLvbMPs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batches = get_batches(encoded_text, 10, 50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7m-d8nSMenD",
        "colab_type": "code",
        "outputId": "84c3096f-333b-4176-deef-7969eab6beb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x,y = next(batches)\n",
        "x.shape, y.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10, 50), (10, 50))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv2OcVUErhKv",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuIk7pYhPV3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, n_input, n_hidden, n_layers, drop_prob):\n",
        "    super(Network, self).__init__()\n",
        "    self.dropout = drop_prob\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_layers = n_layers\n",
        "    self.n_input = n_input\n",
        "    \n",
        "    ### Layers ###\n",
        "    self.rnn1 = nn.LSTM(self.n_input, self.n_hidden, num_layers =self.n_layers, batch_first=True, dropout=self.dropout)\n",
        "    self.dropout = nn.Dropout(self.dropout)\n",
        "    self.fc = nn.Linear(self.n_hidden, self.n_input)\n",
        "    \n",
        "  def forward(self, x, hidden):\n",
        "    x, h = self.rnn1(x, hidden)\n",
        "    x = self.dropout(x)\n",
        "    x = x.contiguous().view(-1, self.n_hidden)\n",
        "    x = self.fc(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    \n",
        "    return x, h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3if0XaRIrNhY",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzLbmXCNhZ0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taken from -> https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb\n",
        "\n",
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = None #net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            if h is not None:\n",
        "              h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = None #net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    if val_h is not None:\n",
        "                      val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj0sroevi5Bj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "ceab0339-20c8-4984-8e5b-ed9365234465"
      },
      "source": [
        "# define and print the net\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "n_chars = len(chars)\n",
        "\n",
        "net = Network(n_chars, n_hidden, n_layers, 0.5)\n",
        "print(net)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (rnn1): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Mkdvs6jbnH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ab1ad06-9084-4dee-8a92-a8af31c9b28c"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded_text, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 2.2381... Val Loss: 2.2050\n",
            "Epoch: 1/20... Step: 20... Loss: 2.1659... Val Loss: 2.1476\n",
            "Epoch: 1/20... Step: 30... Loss: 2.1566... Val Loss: 2.1198\n",
            "Epoch: 1/20... Step: 40... Loss: 2.1065... Val Loss: 2.0998\n",
            "Epoch: 1/20... Step: 50... Loss: 2.1219... Val Loss: 2.0820\n",
            "Epoch: 1/20... Step: 60... Loss: 2.0586... Val Loss: 2.0606\n",
            "Epoch: 1/20... Step: 70... Loss: 2.0660... Val Loss: 2.0467\n",
            "Epoch: 1/20... Step: 80... Loss: 2.0324... Val Loss: 2.0273\n",
            "Epoch: 1/20... Step: 90... Loss: 2.0397... Val Loss: 2.0099\n",
            "Epoch: 1/20... Step: 100... Loss: 2.0015... Val Loss: 1.9964\n",
            "Epoch: 1/20... Step: 110... Loss: 1.9789... Val Loss: 1.9809\n",
            "Epoch: 1/20... Step: 120... Loss: 1.9359... Val Loss: 1.9612\n",
            "Epoch: 1/20... Step: 130... Loss: 1.9780... Val Loss: 1.9422\n",
            "Epoch: 2/20... Step: 140... Loss: 1.9688... Val Loss: 1.9290\n",
            "Epoch: 2/20... Step: 150... Loss: 1.9449... Val Loss: 1.9092\n",
            "Epoch: 2/20... Step: 160... Loss: 1.9438... Val Loss: 1.8944\n",
            "Epoch: 2/20... Step: 170... Loss: 1.9044... Val Loss: 1.8855\n",
            "Epoch: 2/20... Step: 180... Loss: 1.8623... Val Loss: 1.8779\n",
            "Epoch: 2/20... Step: 190... Loss: 1.8258... Val Loss: 1.8470\n",
            "Epoch: 2/20... Step: 200... Loss: 1.8224... Val Loss: 1.8372\n",
            "Epoch: 2/20... Step: 210... Loss: 1.8329... Val Loss: 1.8208\n",
            "Epoch: 2/20... Step: 220... Loss: 1.8100... Val Loss: 1.8107\n",
            "Epoch: 2/20... Step: 230... Loss: 1.8203... Val Loss: 1.7957\n",
            "Epoch: 2/20... Step: 240... Loss: 1.8046... Val Loss: 1.7828\n",
            "Epoch: 2/20... Step: 250... Loss: 1.7626... Val Loss: 1.7693\n",
            "Epoch: 2/20... Step: 260... Loss: 1.7327... Val Loss: 1.7600\n",
            "Epoch: 2/20... Step: 270... Loss: 1.7671... Val Loss: 1.7452\n",
            "Epoch: 3/20... Step: 280... Loss: 1.7484... Val Loss: 1.7365\n",
            "Epoch: 3/20... Step: 290... Loss: 1.7499... Val Loss: 1.7233\n",
            "Epoch: 3/20... Step: 300... Loss: 1.7116... Val Loss: 1.7117\n",
            "Epoch: 3/20... Step: 310... Loss: 1.7173... Val Loss: 1.7042\n",
            "Epoch: 3/20... Step: 320... Loss: 1.6748... Val Loss: 1.6959\n",
            "Epoch: 3/20... Step: 330... Loss: 1.6789... Val Loss: 1.6823\n",
            "Epoch: 3/20... Step: 340... Loss: 1.7166... Val Loss: 1.6738\n",
            "Epoch: 3/20... Step: 350... Loss: 1.6646... Val Loss: 1.6647\n",
            "Epoch: 3/20... Step: 360... Loss: 1.6303... Val Loss: 1.6609\n",
            "Epoch: 3/20... Step: 370... Loss: 1.6621... Val Loss: 1.6503\n",
            "Epoch: 3/20... Step: 380... Loss: 1.6563... Val Loss: 1.6467\n",
            "Epoch: 3/20... Step: 390... Loss: 1.6340... Val Loss: 1.6373\n",
            "Epoch: 3/20... Step: 400... Loss: 1.6073... Val Loss: 1.6253\n",
            "Epoch: 3/20... Step: 410... Loss: 1.6242... Val Loss: 1.6173\n",
            "Epoch: 4/20... Step: 420... Loss: 1.6248... Val Loss: 1.6111\n",
            "Epoch: 4/20... Step: 430... Loss: 1.6198... Val Loss: 1.6061\n",
            "Epoch: 4/20... Step: 440... Loss: 1.6061... Val Loss: 1.5987\n",
            "Epoch: 4/20... Step: 450... Loss: 1.5594... Val Loss: 1.5929\n",
            "Epoch: 4/20... Step: 460... Loss: 1.5402... Val Loss: 1.5856\n",
            "Epoch: 4/20... Step: 470... Loss: 1.6030... Val Loss: 1.5793\n",
            "Epoch: 4/20... Step: 480... Loss: 1.5849... Val Loss: 1.5716\n",
            "Epoch: 4/20... Step: 490... Loss: 1.5836... Val Loss: 1.5671\n",
            "Epoch: 4/20... Step: 500... Loss: 1.5850... Val Loss: 1.5622\n",
            "Epoch: 4/20... Step: 510... Loss: 1.5521... Val Loss: 1.5514\n",
            "Epoch: 4/20... Step: 520... Loss: 1.5671... Val Loss: 1.5521\n",
            "Epoch: 4/20... Step: 530... Loss: 1.5480... Val Loss: 1.5464\n",
            "Epoch: 4/20... Step: 540... Loss: 1.5115... Val Loss: 1.5358\n",
            "Epoch: 4/20... Step: 550... Loss: 1.5709... Val Loss: 1.5358\n",
            "Epoch: 5/20... Step: 560... Loss: 1.5326... Val Loss: 1.5318\n",
            "Epoch: 5/20... Step: 570... Loss: 1.5258... Val Loss: 1.5266\n",
            "Epoch: 5/20... Step: 580... Loss: 1.5163... Val Loss: 1.5216\n",
            "Epoch: 5/20... Step: 590... Loss: 1.5015... Val Loss: 1.5170\n",
            "Epoch: 5/20... Step: 600... Loss: 1.5059... Val Loss: 1.5153\n",
            "Epoch: 5/20... Step: 610... Loss: 1.4840... Val Loss: 1.5037\n",
            "Epoch: 5/20... Step: 620... Loss: 1.4968... Val Loss: 1.5065\n",
            "Epoch: 5/20... Step: 630... Loss: 1.5068... Val Loss: 1.5025\n",
            "Epoch: 5/20... Step: 640... Loss: 1.4779... Val Loss: 1.4981\n",
            "Epoch: 5/20... Step: 650... Loss: 1.4865... Val Loss: 1.4899\n",
            "Epoch: 5/20... Step: 660... Loss: 1.4661... Val Loss: 1.4855\n",
            "Epoch: 5/20... Step: 670... Loss: 1.4828... Val Loss: 1.4831\n",
            "Epoch: 5/20... Step: 680... Loss: 1.4814... Val Loss: 1.4767\n",
            "Epoch: 5/20... Step: 690... Loss: 1.4681... Val Loss: 1.4789\n",
            "Epoch: 6/20... Step: 700... Loss: 1.4709... Val Loss: 1.4710\n",
            "Epoch: 6/20... Step: 710... Loss: 1.4612... Val Loss: 1.4712\n",
            "Epoch: 6/20... Step: 720... Loss: 1.4492... Val Loss: 1.4644\n",
            "Epoch: 6/20... Step: 730... Loss: 1.4734... Val Loss: 1.4619\n",
            "Epoch: 6/20... Step: 740... Loss: 1.4344... Val Loss: 1.4625\n",
            "Epoch: 6/20... Step: 750... Loss: 1.4329... Val Loss: 1.4581\n",
            "Epoch: 6/20... Step: 760... Loss: 1.4696... Val Loss: 1.4572\n",
            "Epoch: 6/20... Step: 770... Loss: 1.4394... Val Loss: 1.4536\n",
            "Epoch: 6/20... Step: 780... Loss: 1.4271... Val Loss: 1.4473\n",
            "Epoch: 6/20... Step: 790... Loss: 1.4274... Val Loss: 1.4438\n",
            "Epoch: 6/20... Step: 800... Loss: 1.4347... Val Loss: 1.4414\n",
            "Epoch: 6/20... Step: 810... Loss: 1.4239... Val Loss: 1.4425\n",
            "Epoch: 6/20... Step: 820... Loss: 1.3834... Val Loss: 1.4335\n",
            "Epoch: 6/20... Step: 830... Loss: 1.4257... Val Loss: 1.4344\n",
            "Epoch: 7/20... Step: 840... Loss: 1.4012... Val Loss: 1.4285\n",
            "Epoch: 7/20... Step: 850... Loss: 1.4101... Val Loss: 1.4291\n",
            "Epoch: 7/20... Step: 860... Loss: 1.3939... Val Loss: 1.4261\n",
            "Epoch: 7/20... Step: 870... Loss: 1.4099... Val Loss: 1.4234\n",
            "Epoch: 7/20... Step: 880... Loss: 1.4128... Val Loss: 1.4225\n",
            "Epoch: 7/20... Step: 890... Loss: 1.4119... Val Loss: 1.4221\n",
            "Epoch: 7/20... Step: 900... Loss: 1.3960... Val Loss: 1.4217\n",
            "Epoch: 7/20... Step: 910... Loss: 1.3773... Val Loss: 1.4184\n",
            "Epoch: 7/20... Step: 920... Loss: 1.3890... Val Loss: 1.4142\n",
            "Epoch: 7/20... Step: 930... Loss: 1.3850... Val Loss: 1.4117\n",
            "Epoch: 7/20... Step: 940... Loss: 1.3862... Val Loss: 1.4104\n",
            "Epoch: 7/20... Step: 950... Loss: 1.4009... Val Loss: 1.4072\n",
            "Epoch: 7/20... Step: 960... Loss: 1.4014... Val Loss: 1.4040\n",
            "Epoch: 7/20... Step: 970... Loss: 1.4011... Val Loss: 1.4041\n",
            "Epoch: 8/20... Step: 980... Loss: 1.3832... Val Loss: 1.4073\n",
            "Epoch: 8/20... Step: 990... Loss: 1.3805... Val Loss: 1.4004\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.3689... Val Loss: 1.3974\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.4037... Val Loss: 1.3958\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.3744... Val Loss: 1.3987\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.3655... Val Loss: 1.3937\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.3815... Val Loss: 1.3936\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.3574... Val Loss: 1.3899\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.3579... Val Loss: 1.3891\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.3626... Val Loss: 1.3852\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.3618... Val Loss: 1.3843\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.3469... Val Loss: 1.3794\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.3386... Val Loss: 1.3790\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.3521... Val Loss: 1.3774\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.3512... Val Loss: 1.3757\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.3635... Val Loss: 1.3762\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.3564... Val Loss: 1.3722\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.3770... Val Loss: 1.3709\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.3392... Val Loss: 1.3731\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.3432... Val Loss: 1.3724\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.3473... Val Loss: 1.3716\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.3805... Val Loss: 1.3686\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.3288... Val Loss: 1.3689\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.3248... Val Loss: 1.3629\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.3266... Val Loss: 1.3639\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.3149... Val Loss: 1.3625\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.3221... Val Loss: 1.3595\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.3322... Val Loss: 1.3562\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.3372... Val Loss: 1.3598\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.3297... Val Loss: 1.3578\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.3530... Val Loss: 1.3557\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.3349... Val Loss: 1.3538\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.3230... Val Loss: 1.3553\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.3325... Val Loss: 1.3550\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.3003... Val Loss: 1.3509\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.2941... Val Loss: 1.3518\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.2895... Val Loss: 1.3510\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.2881... Val Loss: 1.3506\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.3075... Val Loss: 1.3490\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.2869... Val Loss: 1.3464\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.3199... Val Loss: 1.3442\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.3457... Val Loss: 1.3430\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.3305... Val Loss: 1.3449\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.3475... Val Loss: 1.3444\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.3445... Val Loss: 1.3386\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.3042... Val Loss: 1.3427\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.3339... Val Loss: 1.3425\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.2636... Val Loss: 1.3416\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.2960... Val Loss: 1.3365\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.2814... Val Loss: 1.3367\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.3013... Val Loss: 1.3367\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.2951... Val Loss: 1.3364\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.2758... Val Loss: 1.3372\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.2701... Val Loss: 1.3327\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.2940... Val Loss: 1.3332\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.3462... Val Loss: 1.3335\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.3053... Val Loss: 1.3340\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.3107... Val Loss: 1.3290\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.3193... Val Loss: 1.3242\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.2741... Val Loss: 1.3301\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.2458... Val Loss: 1.3282\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.2564... Val Loss: 1.3327\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.2680... Val Loss: 1.3277\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.2665... Val Loss: 1.3257\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.2625... Val Loss: 1.3276\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.2872... Val Loss: 1.3274\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.2652... Val Loss: 1.3274\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.2549... Val Loss: 1.3226\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.3014... Val Loss: 1.3218\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.2778... Val Loss: 1.3206\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.2753... Val Loss: 1.3202\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.2639... Val Loss: 1.3193\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.2679... Val Loss: 1.3168\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.2501... Val Loss: 1.3209\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.2539... Val Loss: 1.3190\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.2883... Val Loss: 1.3209\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.2540... Val Loss: 1.3174\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.2322... Val Loss: 1.3207\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.2634... Val Loss: 1.3152\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.2775... Val Loss: 1.3183\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.2583... Val Loss: 1.3175\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.2337... Val Loss: 1.3145\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.2693... Val Loss: 1.3135\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.2661... Val Loss: 1.3124\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.2593... Val Loss: 1.3104\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.2770... Val Loss: 1.3111\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.2174... Val Loss: 1.3108\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.2126... Val Loss: 1.3115\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.2554... Val Loss: 1.3095\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.2759... Val Loss: 1.3094\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.2561... Val Loss: 1.3034\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.2754... Val Loss: 1.3102\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.2538... Val Loss: 1.3048\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.2448... Val Loss: 1.3041\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.2511... Val Loss: 1.3065\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.2160... Val Loss: 1.3044\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.2738... Val Loss: 1.3042\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.2408... Val Loss: 1.3078\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.2466... Val Loss: 1.3042\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.2469... Val Loss: 1.3007\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.2299... Val Loss: 1.3033\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.2369... Val Loss: 1.3018\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.2185... Val Loss: 1.3009\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.2344... Val Loss: 1.2996\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.2539... Val Loss: 1.2993\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.2219... Val Loss: 1.3008\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.2393... Val Loss: 1.2956\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.2273... Val Loss: 1.2974\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.2462... Val Loss: 1.2986\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.2474... Val Loss: 1.2968\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.2465... Val Loss: 1.2946\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.2449... Val Loss: 1.2987\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.2334... Val Loss: 1.2949\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.2130... Val Loss: 1.2927\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.2400... Val Loss: 1.2951\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.2218... Val Loss: 1.2913\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.2215... Val Loss: 1.2920\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.2563... Val Loss: 1.2922\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2285... Val Loss: 1.2925\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.2193... Val Loss: 1.2956\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.2219... Val Loss: 1.2937\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.2380... Val Loss: 1.2921\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.2159... Val Loss: 1.2879\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.1900... Val Loss: 1.2873\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.2216... Val Loss: 1.2916\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2135... Val Loss: 1.2814\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2135... Val Loss: 1.2794\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.1930... Val Loss: 1.2832\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.2120... Val Loss: 1.2807\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2144... Val Loss: 1.2818\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2246... Val Loss: 1.2776\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.2155... Val Loss: 1.2832\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.1841... Val Loss: 1.2792\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.2023... Val Loss: 1.2843\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2000... Val Loss: 1.2752\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.1979... Val Loss: 1.2737\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.2182... Val Loss: 1.2737\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.2081... Val Loss: 1.2745\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.2199... Val Loss: 1.2776\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.1936... Val Loss: 1.2737\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2028... Val Loss: 1.2734\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.1981... Val Loss: 1.2681\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.2260... Val Loss: 1.2710\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2175... Val Loss: 1.2678\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.1930... Val Loss: 1.2660\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2072... Val Loss: 1.2723\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.1904... Val Loss: 1.2696\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.1817... Val Loss: 1.2693\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2062... Val Loss: 1.2758\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.1899... Val Loss: 1.2750\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.1905... Val Loss: 1.2670\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.1817... Val Loss: 1.2678\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.1843... Val Loss: 1.2689\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.1942... Val Loss: 1.2701\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.1998... Val Loss: 1.2646\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2075... Val Loss: 1.2627\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2155... Val Loss: 1.2645\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.1766... Val Loss: 1.2681\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.1893... Val Loss: 1.2629\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.1829... Val Loss: 1.2677\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2163... Val Loss: 1.2637\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.1753... Val Loss: 1.2659\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.1796... Val Loss: 1.2720\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.1778... Val Loss: 1.2659\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.1741... Val Loss: 1.2669\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.1759... Val Loss: 1.2618\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.1969... Val Loss: 1.2635\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.1884... Val Loss: 1.2625\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.1910... Val Loss: 1.2597\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.1953... Val Loss: 1.2575\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2003... Val Loss: 1.2598\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.1820... Val Loss: 1.2634\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.1883... Val Loss: 1.2576\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.1636... Val Loss: 1.2632\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.1647... Val Loss: 1.2603\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.1469... Val Loss: 1.2591\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.1574... Val Loss: 1.2575\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.1616... Val Loss: 1.2561\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.1594... Val Loss: 1.2550\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.1967... Val Loss: 1.2539\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2229... Val Loss: 1.2589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isYi6ELBrIfz",
        "colab_type": "text"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3RXV5b1jbkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[char2int[char]]])\n",
        "        x = one_hot_encode(x, n_chars)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        if h != None:\n",
        "          h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(n_chars)\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return int2char[char], h\n",
        "      \n",
        "      \n",
        "def sample(net, size, prime='The', top_k=None):\n",
        "    net.cuda()\n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = None\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LrrIIs4jbfI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "14de63b9-b444-482e-8678-7a10f923a3e9"
      },
      "source": [
        "print(sample(net, 1000, prime='There', top_k=None))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Theres to\n",
            "be exceptionally an anbormor red eyes, capticild with nose, cannot be still not\n",
            "trouble--it has to stop, but it was not to ask Kitty. She added their\n",
            "times, and all the kissed his scatelice, and transling lovem at him.\n",
            "\n",
            "\"But I am goaring about, or live you hos cloagh. _Come douese sI dut's\n",
            "a few still so,\" said Metrov. \"I harder thas eyer fised the wedd--not\n",
            "fifting and instants Constious of the shights, like some use,\" she said, with\n",
            "the excate offoring bight's thought.\n",
            "\n",
            "\"She seess for the money at home, and Vronsky, or the ordirate walk. When I beg you\n",
            "to like his wants, and my son after oging you to be-reached.\"\n",
            "\n",
            "\"Buc shouse dos. I feel of my taken, at the complaints! That's even\n",
            "supposent a girl and all in a fool?\" the feeling of tee-inciquance had\n",
            "said to his brother.\n",
            "\n",
            "As he saw all that he was in childres or continually interesting it with her\n",
            "presence.\"\n",
            "\n",
            "\"No, how not another could not get speak of one in the last tenge in\n",
            "herself to give you the doctor, then, to tell me out \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGvbbtmLqskV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "27a8b1fd-e1c7-427d-c129-9c18a11fbdbf"
      },
      "source": [
        "print(sample(net, 1000, prime='Life is beautiful', top_k=None))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Life is beautiful passion, done\n",
            "simply impossible, ease, live on this evening. Before that however view and\n",
            "dinner peace, of straight reloving. It was utterly kissed him; so she was\n",
            "not doing. The family gentleman and Dolly was so that the day beung back.\n",
            "\n",
            "\"Here us?\" he understood the brother.\n",
            "\n",
            "\"Why, well, I'd realute some.\"\n",
            "\n",
            "The strengthe oll broken extraced his new hired-foredread--he stopped him.\n",
            "\n",
            "\"I imagine by the clerl, and _house they signs,\" Konstantin Levin bowed to her.\n",
            "\n",
            "\"Where are you saying?\" said Alexey Alexandrovitch \"tried before Anna's\n",
            "wife's letter; of that was no idea to a humal sund not cleared by those\n",
            "district. He said to equret, humiliated, sympathy with the shage of whether Vronsky had back met the\n",
            "tiny.\"\n",
            "\n",
            "\"I may be terry part in...\n",
            "I can't go\n",
            "on,\" he said, wto merry and rans.\n",
            "\n",
            "\"You paviling you distressed to God, she was!\" said feet said from everything,\n",
            "gosting out, tears. He felt that the year the vesitable new light talked of\n",
            "its armss.\n",
            "\n",
            "On rupking effort; and he asked Alexey A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ4nc8A8q0p8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "917d2ea4-c3a0-49f1-936b-968f16efd473"
      },
      "source": [
        "print(sample(net, 1000, prime='Friendship is like', top_k=None))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Friendship is like\n",
            "at answer.\n",
            "\n",
            "\"The thing charm, is here to Certainly and I see you in the way all the\n",
            "one,\" he said people, and went to the prinw, and understanding her\n",
            "hand, and the daughter's fore addired his same, and her head now silent from\n",
            "when he reached the red conscientions with all his brother natured, and talking at hom,\n",
            "to tell him her in his mind was to sway. He rose of might\n",
            "right in speak in the flode thrieg and deal of more. The sprent\n",
            "that held without meeting, on which, as soon in an ablo of it, but\n",
            "he heared his bent over his brother, with a trouble on her face.\n",
            "\n",
            "\"What! Measure, but I can disnote to detice that?\" he said to himself.\n",
            "\n",
            "\"Let's go.\"\n",
            "\n",
            "\"I have so much in evening. Alexey Alexandrovitch's flipsors on the\n",
            "bidler?\"\n",
            "\n",
            "He said to Alexey Alexandrovitch and his brothers was evident his gave\n",
            "to her turning round, and selical usseas enproaded now on the lome to ckean\n",
            "on the tolr into those years, the interest of the mersiffere\n",
            "to her as if he should nice. And he would not help doy his\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DEoF21Qr2as",
        "colab_type": "text"
      },
      "source": [
        "### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UILKaxBnFl2g",
        "colab_type": "code",
        "outputId": "23bb2a45-c0f9-4957-f256-99dc05737493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "torch.save({'epoch': n_epochs,\n",
        "            'model': Network(n_chars, n_hidden, n_layers, 0.5),\n",
        "            'model_state_dict': net.state_dict()\n",
        "           }, 'char_rnn_2mo.pth.tar')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6br0T9MzMe4J",
        "colab_type": "code",
        "outputId": "a3702407-94c3-44cf-9968-c466a4856e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpt = torch.load('char_rnn_2mo.pth.tar')\n",
        "checkpt"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 20, 'model': Network(\n",
              "   (rnn1): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "   (dropout): Dropout(p=0.5)\n",
              "   (fc): Linear(in_features=512, out_features=83, bias=True)\n",
              " ), 'model_state_dict': OrderedDict([('rnn1.weight_ih_l0',\n",
              "               tensor([[-0.0127,  0.0293,  0.1126,  ...,  0.1241, -0.4026, -0.0992],\n",
              "                       [-0.0396, -0.1046,  0.0485,  ...,  0.3855,  0.2706, -0.1565],\n",
              "                       [-0.2313,  0.4806, -0.0224,  ..., -0.0028,  0.2673, -0.1755],\n",
              "                       ...,\n",
              "                       [-0.0806, -0.4378, -0.1189,  ...,  0.2344, -0.4833,  0.2831],\n",
              "                       [-0.1604,  0.1895, -0.0270,  ...,  0.0176,  0.0566,  0.7817],\n",
              "                       [ 0.1647,  0.0309, -0.0200,  ...,  0.0901,  0.4195, -0.3058]],\n",
              "                      device='cuda:0')),\n",
              "              ('rnn1.weight_hh_l0',\n",
              "               tensor([[ 0.0855, -0.2022,  0.1557,  ..., -0.2245, -0.0040,  0.2665],\n",
              "                       [ 0.0800,  0.0292,  0.1410,  ..., -0.1152,  0.0762,  0.0610],\n",
              "                       [-0.0967,  0.2090,  0.0898,  ...,  0.0634,  0.2185,  0.0429],\n",
              "                       ...,\n",
              "                       [-0.2319,  0.1977,  0.2853,  ...,  0.0008,  0.1781, -0.0149],\n",
              "                       [ 0.0371, -0.2246,  0.3344,  ..., -0.3015, -0.3327,  0.2268],\n",
              "                       [-0.0406, -0.0105,  0.1123,  ...,  0.0643, -0.0192, -0.0096]],\n",
              "                      device='cuda:0')),\n",
              "              ('rnn1.bias_ih_l0',\n",
              "               tensor([0.1911, 0.0527, 0.0523,  ..., 0.1153, 0.2180, 0.0402], device='cuda:0')),\n",
              "              ('rnn1.bias_hh_l0',\n",
              "               tensor([0.1045, 0.0727, 0.0909,  ..., 0.1089, 0.1871, 0.0325], device='cuda:0')),\n",
              "              ('rnn1.weight_ih_l1',\n",
              "               tensor([[ 0.2278,  0.0655,  0.0140,  ...,  0.0765, -0.0453,  0.0575],\n",
              "                       [ 0.1879, -0.2036,  0.0901,  ..., -0.0139, -0.1637, -0.0077],\n",
              "                       [-0.2164,  0.0637,  0.0998,  ...,  0.1019, -0.0783,  0.0390],\n",
              "                       ...,\n",
              "                       [-0.0905,  0.0217,  0.0248,  ...,  0.0624,  0.0583, -0.0988],\n",
              "                       [-0.2590,  0.0666,  0.0055,  ..., -0.0085,  0.2003, -0.1550],\n",
              "                       [-0.1392,  0.0933,  0.0225,  ..., -0.1737, -0.0410, -0.0187]],\n",
              "                      device='cuda:0')),\n",
              "              ('rnn1.weight_hh_l1',\n",
              "               tensor([[ 0.0471, -0.1702, -0.0123,  ..., -0.1147,  0.0283, -0.1658],\n",
              "                       [ 0.0126,  0.0637, -0.0214,  ...,  0.0748, -0.0495,  0.1790],\n",
              "                       [-0.0121,  0.0474, -0.1061,  ...,  0.1934, -0.0273, -0.0411],\n",
              "                       ...,\n",
              "                       [ 0.0411, -0.0616,  0.0746,  ..., -0.0617,  0.0160, -0.1914],\n",
              "                       [-0.2055, -0.1146,  0.1445,  ...,  0.0489,  0.2770, -0.0857],\n",
              "                       [-0.2136, -0.0691, -0.0176,  ..., -0.1236, -0.0471,  0.1113]],\n",
              "                      device='cuda:0')),\n",
              "              ('rnn1.bias_ih_l1',\n",
              "               tensor([ 0.1097,  0.0499,  0.0463,  ..., -0.0350, -0.0834, -0.0706],\n",
              "                      device='cuda:0')),\n",
              "              ('rnn1.bias_hh_l1',\n",
              "               tensor([ 0.0443,  0.1145,  0.0533,  ..., -0.0598, -0.0871, -0.0801],\n",
              "                      device='cuda:0')),\n",
              "              ('fc.weight',\n",
              "               tensor([[-0.0715,  0.1952,  0.0640,  ..., -0.0046,  0.1252,  0.1675],\n",
              "                       [ 0.0735,  0.0026, -0.0287,  ...,  0.0972, -0.0529,  0.4161],\n",
              "                       [-0.1384,  0.0449,  0.2010,  ..., -0.1004,  0.2267,  0.1815],\n",
              "                       ...,\n",
              "                       [-0.0258, -0.0116, -0.0273,  ...,  0.0994, -0.0383, -0.0844],\n",
              "                       [ 0.1312, -0.0012,  0.0262,  ...,  0.0967,  0.0630, -0.0824],\n",
              "                       [-0.0029,  0.0609, -0.0700,  ...,  0.0286, -0.0112, -0.0794]],\n",
              "                      device='cuda:0')),\n",
              "              ('fc.bias',\n",
              "               tensor([-0.1927,  0.0644, -0.2907, -0.1040, -0.1589, -0.2526, -0.1334, -0.2728,\n",
              "                        0.0050, -0.1963, -0.0926, -0.0216,  0.0746, -0.4441, -0.3784, -0.1114,\n",
              "                       -0.1115,  0.0998, -0.2614, -0.3438, -0.3597, -0.3228, -0.3373, -0.1277,\n",
              "                       -0.2262, -0.2307, -0.2841, -0.0359, -0.2930, -0.3454, -0.3454,  0.0116,\n",
              "                       -0.2521, -0.2045, -0.2278, -0.2423, -0.2018, -0.0926, -0.2453, -0.2987,\n",
              "                       -0.4050, -0.2371, -0.2201, -0.0795,  0.0335, -0.1614,  0.0179, -0.3138,\n",
              "                       -0.2594,  0.0270, -0.3823, -0.2150, -0.2603, -0.3287, -0.3335,  0.1590,\n",
              "                       -0.3212, -0.3354, -0.3480,  0.1352, -0.3150, -0.2605, -0.1303, -0.2481,\n",
              "                       -0.3570, -0.2171, -0.2440, -0.2070, -0.0174,  0.2076, -0.0368,  0.2021,\n",
              "                       -0.0290, -0.2422, -0.3889, -0.1918, -0.2787, -0.3410, -0.4166, -0.3610,\n",
              "                        0.2030,  0.0742, -0.1925], device='cuda:0'))])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWQ9ACFB8Alj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = checkpt['epoch']\n",
        "model = checkpt['model']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SutYbZe8Vwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(checkpt['model_state_dict'])\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFBDeZVA8tmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "246bb3db-4770-4b2c-9cbb-6f35b982c171"
      },
      "source": [
        "print(sample(net, 1000, prime='Friendship is like', top_k=None))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Friendship is like stuly as realize. He\n",
            "began to gre with them from her love with Petersburg, and they was loading\n",
            "has the compressing bold and rather unurt of her tone.\n",
            "\n",
            "The lessons let the yead away round her in the dost\n",
            "conversation with it, did not suppose when Dolly wanted to love him,\n",
            "and the strimms of another first, strung Them had had been sportally.n\n",
            "\n",
            "\"Well, then it may be barred all to be wearyed?\" said Sviazhsky's woman; \"that you're\n",
            "at the same gied in the table capital things, and as, Vronsky had, uncapbered his\n",
            "resurve more of public quist feeling. I will srop sweet sungen\n",
            "the one another little--everything it told. And Afferond like\n",
            "patient, for God. Who said: I have disapprienced I am to turn to my plans. For the\n",
            "princess.\n",
            "\n",
            "\"Well,. Can man you comaring it. I can't being it!\"\n",
            "\n",
            "Stepan Arkadyevitch was said to her.\n",
            "\n",
            "And gown up he was talking to the samication, was shally and rojeeved, in\n",
            "his children) and shappaned to see the paces too the way that\n",
            "he went on, and stepping short to what wa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah4B_aV0sbro",
        "colab_type": "text"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-eUn_7YshWM",
        "colab_type": "text"
      },
      "source": [
        "- Correct Spellings: Most of the words in the context won't make much sense but they have correct spellings. The model learnt all of it in just 5-7 mins of training. This is amazing.\n",
        "- Opening and closing qoutes (\" \"): The model has also learnt to that \"quotes\" comes in pair. \n",
        "- Using punctuations: the model is also using a lot of punctuations and most of them looks appropriate to me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcC51HjGt2ky",
        "colab_type": "text"
      },
      "source": [
        "### Further Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqmjSyWct6B-",
        "colab_type": "text"
      },
      "source": [
        "- [Karpathy's blog on RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
      ]
    }
  ]
}